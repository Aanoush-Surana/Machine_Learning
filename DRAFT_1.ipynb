{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM63WW/5Hqxc3i8LuhMoRAs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aanoush-Surana/Machine_Learning/blob/main/DRAFT_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPU"
      ],
      "metadata": {
        "id": "sH6_5IqPIF3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n",
        "\n",
        "!pip install -U torch torchvision torchaudio\n",
        "!pip install -U 'git+https://github.com/facebookresearch/detectron2.git'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SiwwfgZlIIkZ",
        "outputId": "e1aef9e9-7b37-40f8-d8b6-3d9f8c6cba41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Feb  3 13:04:09 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.10.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.25.0)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: cuda-bindings==12.9.4 in /usr/local/lib/python3.12/dist-packages (from torch) (12.9.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.6.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.0)\n",
            "Requirement already satisfied: cuda-pathfinder~=1.1 in /usr/local/lib/python3.12/dist-packages (from cuda-bindings==12.9.4->torch) (1.3.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Collecting git+https://github.com/facebookresearch/detectron2.git\n",
            "  Cloning https://github.com/facebookresearch/detectron2.git to /tmp/pip-req-build-cl7_nu2g\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-cl7_nu2g\n",
            "  Resolved https://github.com/facebookresearch/detectron2.git to commit fd27788985af0f4ca800bca563acdb700bb890e2\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (11.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (3.10.0)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (2.0.11)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (3.3.0)\n",
            "Collecting yacs>=0.1.8 (from detectron2==0.6)\n",
            "  Using cached yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (0.9.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (3.1.2)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (4.67.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (2.19.0)\n",
            "Collecting fvcore<0.1.6,>=0.1.5 (from detectron2==0.6)\n",
            "  Using cached fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting iopath<0.1.10,>=0.1.7 (from detectron2==0.6)\n",
            "  Using cached iopath-0.1.9-py3-none-any.whl.metadata (370 bytes)\n",
            "Requirement already satisfied: omegaconf<2.4,>=2.1 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (2.3.0)\n",
            "Collecting hydra-core>=1.1 (from detectron2==0.6)\n",
            "  Using cached hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting black (from detectron2==0.6)\n",
            "  Downloading black-26.1.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (88 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m88.9/88.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (25.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (2.0.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (6.0.3)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from hydra-core>=1.1->detectron2==0.6) (4.9.3)\n",
            "Collecting portalocker (from iopath<0.1.10,>=0.1.7->detectron2==0.6)\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from black->detectron2==0.6) (8.3.1)\n",
            "Collecting mypy-extensions>=0.4.3 (from black->detectron2==0.6)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting pathspec>=1.0.0 (from black->detectron2==0.6)\n",
            "  Downloading pathspec-1.0.4-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.12/dist-packages (from black->detectron2==0.6) (4.5.1)\n",
            "Collecting pytokens>=0.3.0 (from black->detectron2==0.6)\n",
            "  Downloading pytokens-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (2.9.0.post0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (1.76.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (3.10.1)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (5.29.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (3.1.5)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio>=1.48.2->tensorboard->detectron2==0.6) (4.15.0)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard->detectron2==0.6) (3.0.3)\n",
            "Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
            "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading black-26.1.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Downloading pathspec-1.0.4-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m55.2/55.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytokens-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (269 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m269.8/269.8 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Building wheels for collected packages: detectron2, fvcore\n",
            "  Building wheel for detectron2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for detectron2: filename=detectron2-0.6-cp312-cp312-linux_x86_64.whl size=7107784 sha256=983b4c571fe5f0cc1011007965dc6b1af1f04e1ff71489b90d9f336ec3611637\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-rg89g_nv/wheels/d3/6e/bd/1969578f1456a6be2d6f083da65c669f450b23b8f3d1ac14c1\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61397 sha256=075cac7793dba2cb3365ddd9e8ef1083678bd7330325e166e6e74231248b9e02\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/9f/a5/e4f5b27454ccd4596bd8b62432c7d6b1ca9fa22aef9d70a16a\n",
            "Successfully built detectron2 fvcore\n",
            "Installing collected packages: yacs, pytokens, portalocker, pathspec, mypy-extensions, iopath, hydra-core, black, fvcore, detectron2\n",
            "Successfully installed black-26.1.0 detectron2-0.6 fvcore-0.1.5.post20221221 hydra-core-1.3.2 iopath-0.1.9 mypy-extensions-1.1.0 pathspec-1.0.4 portalocker-3.2.0 pytokens-0.4.1 yacs-0.1.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MOUNT DRIVE"
      ],
      "metadata": {
        "id": "MtvlzIODILGw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3xKrFcgHCXe",
        "outputId": "542173ca-f23b-4eab-ac57-a4a2f60d34fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DEFINE PATHS"
      ],
      "metadata": {
        "id": "18BsfZnwtlfF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "\n",
        "# Your data in Google Drive\n",
        "IMG_ROOT = '/content/drive/MyDrive/idd20kII/leftImg8bit/train'\n",
        "GT_ROOT = '/content/drive/MyDrive/idd20kII/gtFine/train'\n",
        "\n",
        "# Output directories (in Colab temporary storage)\n",
        "PAN_ROOT = '/content/idd_panoptic/train'\n",
        "SEM_ROOT = '/content/idd_semantic/train'\n",
        "JSON_OUT = '/content/panoptic_train.json'\n",
        "OUTPUT_DIR = '/content/output'\n",
        "\n",
        "# Create all directories\n",
        "for path in [PAN_ROOT, SEM_ROOT, OUTPUT_DIR]:\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "# Processing limit (set to None for full dataset)\n",
        "LIMIT = 50\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"PATHS CONFIGURED\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Images:    {IMG_ROOT}\")\n",
        "print(f\"GT JSON:   {GT_ROOT}\")\n",
        "print(f\"Panoptic:  {PAN_ROOT}\")\n",
        "print(f\"Semantic:  {SEM_ROOT}\")\n",
        "print(f\"JSON:      {JSON_OUT}\")\n",
        "print(f\"Output:    {OUTPUT_DIR}\")\n",
        "print(f\"Limit:     {LIMIT}\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vijzg5fytnKB",
        "outputId": "c8dce615-7447-4d42-c791-1c603d68974b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "PATHS CONFIGURED\n",
            "======================================================================\n",
            "Images:    /content/drive/MyDrive/idd20kII/leftImg8bit/train\n",
            "GT JSON:   /content/drive/MyDrive/idd20kII/gtFine/train\n",
            "Panoptic:  /content/idd_panoptic/train\n",
            "Semantic:  /content/idd_semantic/train\n",
            "JSON:      /content/panoptic_train.json\n",
            "Output:    /content/output\n",
            "Limit:     50\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "VERIFY"
      ],
      "metadata": {
        "id": "ZH2P1SrattHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CHECKING YOUR DATA\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Check images\n",
        "if os.path.isdir(IMG_ROOT):\n",
        "    img_cities = [d for d in os.listdir(IMG_ROOT) if os.path.isdir(os.path.join(IMG_ROOT, d))]\n",
        "    print(f\"\\n‚úì Images: {len(img_cities)} cities found\")\n",
        "    if img_cities:\n",
        "        sample = os.path.join(IMG_ROOT, img_cities[0])\n",
        "        imgs = [f for f in os.listdir(sample) if f.endswith('.jpg')]\n",
        "        print(f\"  Example: {img_cities[0]} has {len(imgs)} images\")\n",
        "        if imgs:\n",
        "            print(f\"  Sample: {imgs[0]}\")\n",
        "else:\n",
        "    raise FileNotFoundError(f\"IMG_ROOT not found: {IMG_ROOT}\")\n",
        "\n",
        "# Check GT JSON files\n",
        "if os.path.isdir(GT_ROOT):\n",
        "    gt_cities = [d for d in os.listdir(GT_ROOT) if os.path.isdir(os.path.join(GT_ROOT, d))]\n",
        "    print(f\"\\n‚úì Ground Truth: {len(gt_cities)} cities found\")\n",
        "    if gt_cities:\n",
        "        sample = os.path.join(GT_ROOT, gt_cities[0])\n",
        "        jsons = [f for f in os.listdir(sample) if f.endswith('_polygons.json')]\n",
        "        print(f\"  Example: {gt_cities[0]} has {len(jsons)} JSON files\")\n",
        "        if jsons:\n",
        "            print(f\"  Sample: {jsons[0]}\")\n",
        "else:\n",
        "    raise FileNotFoundError(f\"GT_ROOT not found: {GT_ROOT}\")\n",
        "\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uyl1Yx2tvE1",
        "outputId": "d9ef3989-aa71-4cab-e80b-467d75b8c373"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "CHECKING YOUR DATA\n",
            "======================================================================\n",
            "\n",
            "‚úì Images: 249 cities found\n",
            "  Example: 201 has 11 images\n",
            "  Sample: frame1979_leftImg8bit.jpg\n",
            "\n",
            "‚úì Ground Truth: 249 cities found\n",
            "  Example: 203 has 24 JSON files\n",
            "  Sample: frame2605_gtFine_polygons.json\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CREATE PANOPTIC + SEMANTIC MASKS"
      ],
      "metadata": {
        "id": "pL_X6QCHuAFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CREATING PANOPTIC & SEMANTIC MASKS FROM POLYGON JSON\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "def polygon_to_mask(polygons, img_height, img_width, instance_id):\n",
        "    \"\"\"\n",
        "    Convert polygon coordinates to a binary mask with given instance_id.\n",
        "\n",
        "    Args:\n",
        "        polygons: List of polygon coordinates [[x1,y1,x2,y2,...], ...]\n",
        "        img_height, img_width: Image dimensions\n",
        "        instance_id: Unique ID for this instance\n",
        "\n",
        "    Returns:\n",
        "        mask: numpy array with instance_id where polygon is, 0 elsewhere\n",
        "    \"\"\"\n",
        "    mask = np.zeros((img_height, img_width), dtype=np.int32)\n",
        "\n",
        "    for poly in polygons:\n",
        "        if len(poly) < 6:  # Need at least 3 points (6 coordinates)\n",
        "            continue\n",
        "\n",
        "        # Reshape to [(x,y), (x,y), ...] format\n",
        "        points = np.array(poly).reshape(-1, 2).astype(np.int32)\n",
        "\n",
        "        # Fill polygon\n",
        "        cv2.fillPoly(mask, [points], instance_id)\n",
        "\n",
        "    return mask\n",
        "\n",
        "# Storage for dataset\n",
        "images_list = []\n",
        "annotations_list = []\n",
        "img_id = 0\n",
        "total_processed = 0\n",
        "total_skipped = 0\n",
        "total_instances = 0\n",
        "\n",
        "# Process each city\n",
        "for city in sorted(os.listdir(GT_ROOT)):\n",
        "    city_gt = os.path.join(GT_ROOT, city)\n",
        "    city_img = os.path.join(IMG_ROOT, city)\n",
        "    city_pan = os.path.join(PAN_ROOT, city)\n",
        "    city_sem = os.path.join(SEM_ROOT, city)\n",
        "\n",
        "    if not os.path.isdir(city_gt):\n",
        "        continue\n",
        "\n",
        "    # Create output directories for this city\n",
        "    os.makedirs(city_pan, exist_ok=True)\n",
        "    os.makedirs(city_sem, exist_ok=True)\n",
        "\n",
        "    print(f\"\\nProcessing city: {city}\")\n",
        "\n",
        "    # Process each JSON file in this city\n",
        "    for json_file in sorted(os.listdir(city_gt)):\n",
        "        if not json_file.endswith('_gtFine_polygons.json'):\n",
        "            continue\n",
        "\n",
        "        # Check limit\n",
        "        if LIMIT is not None and total_processed >= LIMIT:\n",
        "            break\n",
        "\n",
        "        # Extract base name: frame1979_gtFine_polygons.json -> frame1979\n",
        "        base_name = json_file.replace('_gtFine_polygons.json', '')\n",
        "\n",
        "        # Corresponding image file\n",
        "        img_name = base_name + '_leftImg8bit.jpg'\n",
        "        img_path = os.path.join(city_img, img_name)\n",
        "\n",
        "        if not os.path.exists(img_path):\n",
        "            print(f\"  ‚ö†Ô∏è  Skip: {img_name} not found\")\n",
        "            total_skipped += 1\n",
        "            continue\n",
        "\n",
        "        # Load image to get dimensions\n",
        "        img = Image.open(img_path)\n",
        "        img_width, img_height = img.size\n",
        "\n",
        "        # Load JSON annotations\n",
        "        json_path = os.path.join(city_gt, json_file)\n",
        "        with open(json_path, 'r') as f:\n",
        "            gt_data = json.load(f)\n",
        "\n",
        "        # Create panoptic mask (each instance gets unique ID)\n",
        "        panoptic_mask = np.zeros((img_height, img_width), dtype=np.uint32)\n",
        "        segments_info = []\n",
        "        instance_id = 1\n",
        "\n",
        "        # Process each object in the JSON\n",
        "        for obj in gt_data.get('objects', []):\n",
        "            # Get polygon coordinates\n",
        "            polygon = obj.get('polygon', [])\n",
        "\n",
        "            if len(polygon) < 3:  # Need at least 3 points\n",
        "                continue\n",
        "\n",
        "            # Convert polygon to mask for this instance\n",
        "            instance_mask = polygon_to_mask([polygon], img_height, img_width, instance_id)\n",
        "\n",
        "            # Add to panoptic mask\n",
        "            panoptic_mask[instance_mask == instance_id] = instance_id\n",
        "\n",
        "            # Calculate bbox and area\n",
        "            ys, xs = np.where(instance_mask == instance_id)\n",
        "            if len(xs) == 0:\n",
        "                continue\n",
        "\n",
        "            x0, x1 = int(xs.min()), int(xs.max())\n",
        "            y0, y1 = int(ys.min()), int(ys.max())\n",
        "            area = int(len(xs))\n",
        "\n",
        "            # Add segment info\n",
        "            segments_info.append({\n",
        "                'id': int(instance_id),\n",
        "                'category_id': 1,  # All objects are category 1\n",
        "                'area': area,\n",
        "                'bbox': [x0, y0, x1 - x0 + 1, y1 - y0 + 1],\n",
        "                'iscrowd': 0\n",
        "            })\n",
        "\n",
        "            instance_id += 1\n",
        "            total_instances += 1\n",
        "\n",
        "        # Create semantic mask (all instances of same class get same ID)\n",
        "        semantic_mask = np.zeros((img_height, img_width), dtype=np.uint8)\n",
        "        for seg in segments_info:\n",
        "            inst_id = seg['id']\n",
        "            cat_id = seg['category_id']\n",
        "            semantic_mask[panoptic_mask == inst_id] = cat_id\n",
        "\n",
        "        # Save panoptic mask\n",
        "        pan_name = base_name + '_leftImg8bit.png'\n",
        "        pan_path = os.path.join(city_pan, pan_name)\n",
        "        Image.fromarray(panoptic_mask).save(pan_path)\n",
        "\n",
        "        # Save semantic mask\n",
        "        sem_name = base_name + '_leftImg8bit.png'\n",
        "        sem_path = os.path.join(city_sem, sem_name)\n",
        "        Image.fromarray(semantic_mask).save(sem_path)\n",
        "\n",
        "        # Add to dataset\n",
        "        images_list.append({\n",
        "            'id': img_id,\n",
        "            'file_name': city + '/' + img_name,\n",
        "            'height': img_height,\n",
        "            'width': img_width\n",
        "        })\n",
        "\n",
        "        annotations_list.append({\n",
        "            'image_id': img_id,\n",
        "            'file_name': city + '/' + pan_name,\n",
        "            'segments_info': segments_info\n",
        "        })\n",
        "\n",
        "        total_processed += 1\n",
        "        if total_processed % 10 == 0:\n",
        "            print(f\"  ‚úì Processed {total_processed} images, {total_instances} instances...\")\n",
        "\n",
        "        img_id += 1\n",
        "\n",
        "    if LIMIT is not None and total_processed >= LIMIT:\n",
        "        break\n",
        "\n",
        "# Create COCO panoptic format JSON\n",
        "panoptic_json = {\n",
        "    'images': images_list,\n",
        "    'annotations': annotations_list,\n",
        "    'categories': [\n",
        "        {\n",
        "            'id': 0,\n",
        "            'name': 'background',\n",
        "            'supercategory': 'void',\n",
        "            'isthing': 0,\n",
        "            'color': [0, 0, 0]\n",
        "        },\n",
        "        {\n",
        "            'id': 1,\n",
        "            'name': 'object',\n",
        "            'supercategory': 'thing',\n",
        "            'isthing': 1,\n",
        "            'color': [220, 20, 60]\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Save JSON\n",
        "with open(JSON_OUT, 'w') as f:\n",
        "    json.dump(panoptic_json, f, indent=2)\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"‚úÖ MASK GENERATION COMPLETE!\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"Images processed:  {total_processed}\")\n",
        "print(f\"Images skipped:    {total_skipped}\")\n",
        "print(f\"Total instances:   {total_instances}\")\n",
        "print(f\"Avg per image:     {total_instances/max(total_processed,1):.1f}\")\n",
        "print(f\"\\nFiles created:\")\n",
        "print(f\"  Panoptic masks ‚Üí {PAN_ROOT}\")\n",
        "print(f\"  Semantic masks ‚Üí {SEM_ROOT}\")\n",
        "print(f\"  JSON metadata  ‚Üí {JSON_OUT}\")\n",
        "print(f\"{'='*70}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Pt7fkYsluEpn",
        "outputId": "55afdb77-66a9-4136-c7ad-18fcbf3b12af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "CREATING PANOPTIC & SEMANTIC MASKS FROM POLYGON JSON\n",
            "======================================================================\n",
            "\n",
            "Processing city: 201\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1117777425.py:140: DeprecationWarning: Saving I mode images as PNG is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
            "  Image.fromarray(panoptic_mask).save(pan_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ‚úì Processed 10 images, 811 instances...\n",
            "\n",
            "Processing city: 202\n",
            "\n",
            "Processing city: 203\n",
            "  ‚úì Processed 20 images, 1290 instances...\n",
            "  ‚úì Processed 30 images, 1634 instances...\n",
            "\n",
            "Processing city: 204\n",
            "  ‚úì Processed 40 images, 2302 instances...\n",
            "  ‚úì Processed 50 images, 3129 instances...\n",
            "\n",
            "======================================================================\n",
            "‚úÖ MASK GENERATION COMPLETE!\n",
            "======================================================================\n",
            "Images processed:  50\n",
            "Images skipped:    0\n",
            "Total instances:   3129\n",
            "Avg per image:     62.6\n",
            "\n",
            "Files created:\n",
            "  Panoptic masks ‚Üí /content/idd_panoptic/train\n",
            "  Semantic masks ‚Üí /content/idd_semantic/train\n",
            "  JSON metadata  ‚Üí /content/panoptic_train.json\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATA MAPPING"
      ],
      "metadata": {
        "id": "wZiriIJcuZr9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import copy\n",
        "import torch\n",
        "import numpy as np\n",
        "from detectron2.data import detection_utils as utils\n",
        "from detectron2.data import transforms as T\n",
        "from detectron2.structures import BitMasks\n",
        "\n",
        "class PanopticDataMapper:\n",
        "    \"\"\"\n",
        "    Custom data mapper that loads images, semantic masks, and annotations.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cfg, is_train=True):\n",
        "        self.is_train = is_train\n",
        "        self.image_format = cfg.INPUT.FORMAT\n",
        "\n",
        "        # Build augmentations\n",
        "        if is_train:\n",
        "            self.augmentations = T.AugmentationList([\n",
        "                T.ResizeShortestEdge(\n",
        "                    cfg.INPUT.MIN_SIZE_TRAIN,\n",
        "                    cfg.INPUT.MAX_SIZE_TRAIN,\n",
        "                    cfg.INPUT.MIN_SIZE_TRAIN_SAMPLING\n",
        "                ),\n",
        "                T.RandomFlip(prob=0.5, horizontal=True, vertical=False)\n",
        "            ])\n",
        "        else:\n",
        "            self.augmentations = T.AugmentationList([\n",
        "                T.ResizeShortestEdge(\n",
        "                    cfg.INPUT.MIN_SIZE_TEST,\n",
        "                    cfg.INPUT.MAX_SIZE_TEST,\n",
        "                    \"choice\"\n",
        "                )\n",
        "            ])\n",
        "\n",
        "    def __call__(self, dataset_dict):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dataset_dict: metadata from the dataset\n",
        "\n",
        "        Returns:\n",
        "            dict with image, sem_seg, and instances\n",
        "        \"\"\"\n",
        "        dataset_dict = copy.deepcopy(dataset_dict)\n",
        "\n",
        "        # 1. Load image\n",
        "        image_path = os.path.join(IMG_ROOT, dataset_dict['file_name'])\n",
        "        image = utils.read_image(image_path, format=self.image_format)\n",
        "        utils.check_image_size(dataset_dict, image)\n",
        "\n",
        "        # 2. Load semantic segmentation mask\n",
        "        sem_file = dataset_dict['file_name'].replace('.jpg', '.png')\n",
        "        sem_path = os.path.join(SEM_ROOT, sem_file)\n",
        "\n",
        "        if not os.path.exists(sem_path):\n",
        "            raise FileNotFoundError(f\"Semantic mask not found: {sem_path}\")\n",
        "\n",
        "        sem_seg_gt = utils.read_image(sem_path, \"L\").squeeze(2)\n",
        "        # 2B. Load panoptic mask (instance ids)\n",
        "        pan_file = dataset_dict['pan_seg_file_name']\n",
        "        pan_path = os.path.join(PAN_ROOT, pan_file)\n",
        "        pan_mask = utils.read_image(pan_path, \"L\").squeeze(2)\n",
        "\n",
        "        # 3. Apply augmentations\n",
        "        aug_input = T.AugInput(image, sem_seg=sem_seg_gt)\n",
        "        transforms = self.augmentations(aug_input)\n",
        "        image = aug_input.image\n",
        "        sem_seg_gt = aug_input.sem_seg\n",
        "\n",
        "        # 4. Convert to tensors\n",
        "        image_shape = image.shape[:2]\n",
        "        dataset_dict['image'] = torch.as_tensor(\n",
        "            np.ascontiguousarray(image.transpose(2, 0, 1))\n",
        "        )\n",
        "        dataset_dict['sem_seg'] = torch.as_tensor(\n",
        "            sem_seg_gt.astype('long')\n",
        "        )\n",
        "\n",
        "        # 5. Process instance annotations + masks\n",
        "        if 'annotations' in dataset_dict:\n",
        "            annos = [\n",
        "                utils.transform_instance_annotations(\n",
        "                    obj, transforms, image_shape\n",
        "                )\n",
        "                for obj in dataset_dict.pop('annotations')\n",
        "                if obj.get('iscrowd', 0) == 0\n",
        "            ]\n",
        "\n",
        "            instances = utils.annotations_to_instances(annos, image_shape)\n",
        "\n",
        "            # ---- ADD INSTANCE MASKS ----\n",
        "            masks = []\n",
        "            for i in range(len(annos)):\n",
        "                inst_id = i + 1\n",
        "                binary_mask = (pan_mask == inst_id).astype(\"uint8\")\n",
        "                masks.append(torch.from_numpy(binary_mask))\n",
        "\n",
        "            if len(masks) > 0:\n",
        "                instances.gt_masks = BitMasks(torch.stack(masks))\n",
        "            # ----------------------------\n",
        "\n",
        "            dataset_dict['instances'] = utils.filter_empty_instances(instances)\n",
        "\n",
        "\n",
        "        return dataset_dict\n",
        "\n",
        "print(\"‚úì Custom data mapper defined\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18wOuhgxub0i",
        "outputId": "e3e7d996-fec6-4d40-a0d3-0778de83b30a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Custom data mapper defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "REGISTER DATASET"
      ],
      "metadata": {
        "id": "4Ub_VxEXurwy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
        "\n",
        "def load_panoptic_dataset():\n",
        "    \"\"\"\n",
        "    Load the panoptic dataset from JSON.\n",
        "    Returns list of dataset dicts in Detectron2 format.\n",
        "    \"\"\"\n",
        "    with open(JSON_OUT, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    dataset_dicts = []\n",
        "\n",
        "    for img_info, ann_info in zip(data['images'], data['annotations']):\n",
        "        record = {\n",
        "            'file_name': img_info['file_name'],\n",
        "            'image_id': img_info['id'],\n",
        "            'height': img_info['height'],\n",
        "            'width': img_info['width'],\n",
        "            'pan_seg_file_name': ann_info['file_name'],\n",
        "            'segments_info': ann_info['segments_info']\n",
        "        }\n",
        "\n",
        "        # Convert segments to annotations for instance detection\n",
        "        annotations = []\n",
        "        for seg in ann_info['segments_info']:\n",
        "            if seg['category_id'] > 0:  # Skip background\n",
        "                annotations.append({\n",
        "                    'bbox': seg['bbox'],\n",
        "                    'bbox_mode': 0,  # XYXY_ABS mode\n",
        "                    'category_id': seg['category_id'] - 1,  # 0-indexed (1->0)\n",
        "                    'iscrowd': seg.get('iscrowd', 0)\n",
        "                })\n",
        "\n",
        "        record['annotations'] = annotations\n",
        "        dataset_dicts.append(record)\n",
        "\n",
        "    return dataset_dicts\n",
        "\n",
        "# Register dataset\n",
        "dataset_name = \"idd_panoptic_train\"\n",
        "DatasetCatalog.register(dataset_name, load_panoptic_dataset)\n",
        "MetadataCatalog.get(dataset_name).set(\n",
        "    thing_classes=[\"object\"],\n",
        "    stuff_classes=[\"background\"],\n",
        "    thing_dataset_id_to_contiguous_id={1: 0},\n",
        "    stuff_dataset_id_to_contiguous_id={0: 0},\n",
        "    image_root=IMG_ROOT,\n",
        "    panoptic_root=PAN_ROOT,\n",
        "    sem_seg_root=SEM_ROOT,\n",
        "    panoptic_json=JSON_OUT,\n",
        "    evaluator_type=\"coco_panoptic_seg\",\n",
        ")\n",
        "\n",
        "print(f\"‚úì Dataset registered: {dataset_name}\")\n",
        "\n",
        "# Verify dataset is not empty\n",
        "dataset_dicts = DatasetCatalog.get(dataset_name)\n",
        "print(f\"  Dataset size: {len(dataset_dicts)} images\")\n",
        "if len(dataset_dicts) == 0:\n",
        "    raise ValueError(\"Dataset is empty! Check CELL 4 for errors.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtVRSZN1uxhn",
        "outputId": "eaa455b1-9dcb-42cf-d039-a52a163d8d6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Dataset registered: idd_panoptic_train\n",
            "  Dataset size: 50 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CONFIGURE MODEL"
      ],
      "metadata": {
        "id": "C2EGstYXu22Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from detectron2.config import get_cfg\n",
        "from detectron2 import model_zoo\n",
        "\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(\n",
        "    model_zoo.get_config_file(\"COCO-PanopticSegmentation/panoptic_fpn_R_50_3x.yaml\")\n",
        ")\n",
        "\n",
        "# Dataset\n",
        "cfg.DATASETS.TRAIN = (dataset_name,)\n",
        "cfg.DATASETS.TEST = ()\n",
        "\n",
        "# Dataloader\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "\n",
        "# Solver\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "cfg.SOLVER.BASE_LR = 0.00025\n",
        "cfg.SOLVER.MAX_ITER = 100\n",
        "cfg.SOLVER.STEPS = []\n",
        "cfg.SOLVER.CHECKPOINT_PERIOD = 50\n",
        "\n",
        "# Model\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # 1 thing class\n",
        "cfg.MODEL.SEM_SEG_HEAD.NUM_CLASSES = 2  # background + object\n",
        "\n",
        "# Output\n",
        "cfg.OUTPUT_DIR = OUTPUT_DIR\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"‚úì Model configured\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Dataset:           {cfg.DATASETS.TRAIN}\")\n",
        "print(f\"Batch size:        {cfg.SOLVER.IMS_PER_BATCH}\")\n",
        "print(f\"Learning rate:     {cfg.SOLVER.BASE_LR}\")\n",
        "print(f\"Max iterations:    {cfg.SOLVER.MAX_ITER}\")\n",
        "print(f\"ROI classes:       {cfg.MODEL.ROI_HEADS.NUM_CLASSES}\")\n",
        "print(f\"Semantic classes:  {cfg.MODEL.SEM_SEG_HEAD.NUM_CLASSES}\")\n",
        "print(f\"Output directory:  {cfg.OUTPUT_DIR}\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtqkmEgcu2WY",
        "outputId": "2c8350b4-fe05-4a88-a79e-3fd9d89cf188"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "‚úì Model configured\n",
            "======================================================================\n",
            "Dataset:           ('idd_panoptic_train',)\n",
            "Batch size:        2\n",
            "Learning rate:     0.00025\n",
            "Max iterations:    100\n",
            "ROI classes:       1\n",
            "Semantic classes:  2\n",
            "Output directory:  /content/output\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRAINER"
      ],
      "metadata": {
        "id": "wzPlZCCWvAew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.data import build_detection_train_loader\n",
        "\n",
        "class PanopticTrainer(DefaultTrainer):\n",
        "    \"\"\"\n",
        "    Custom trainer that uses PanopticDataMapper.\n",
        "    \"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    def build_train_loader(cls, cfg):\n",
        "        mapper = PanopticDataMapper(cfg, is_train=True)\n",
        "        return build_detection_train_loader(cfg, mapper=mapper)\n",
        "\n",
        "print(\"‚úì Custom trainer defined\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IA-4aS88vCdN",
        "outputId": "275d4c2d-de68-4e52-b137-3943e876b03f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Custom trainer defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRAINING"
      ],
      "metadata": {
        "id": "vK8rwnNVvITh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üöÄ STARTING TRAINING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "trainer = PanopticTrainer(cfg)\n",
        "trainer.resume_or_load(resume=False)\n",
        "\n",
        "# Train\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ TRAINING COMPLETE!\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Model saved to: {cfg.OUTPUT_DIR}\")\n",
        "print(f\"Check for model_final.pth\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhKhYKxZvJpS",
        "outputId": "ece1b1fd-aa82-49f4-c939-14542b9f8272"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "üöÄ STARTING TRAINING\n",
            "======================================================================\n",
            "[02/03 14:02:26 d2.engine.defaults]: Model:\n",
            "PanopticFPN(\n",
            "  (backbone): FPN(\n",
            "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (top_block): LastLevelMaxPool()\n",
            "    (bottom_up): ResNet(\n",
            "      (stem): BasicStem(\n",
            "        (conv1): Conv2d(\n",
            "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (res2): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res3): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res4): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (4): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (5): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res5): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (proposal_generator): RPN(\n",
            "    (rpn_head): StandardRPNHead(\n",
            "      (conv): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (anchor_generator): DefaultAnchorGenerator(\n",
            "      (cell_anchors): BufferList()\n",
            "    )\n",
            "  )\n",
            "  (roi_heads): StandardROIHeads(\n",
            "    (box_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (box_head): FastRCNNConvFCHead(\n",
            "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
            "      (fc_relu1): ReLU()\n",
            "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (fc_relu2): ReLU()\n",
            "    )\n",
            "    (box_predictor): FastRCNNOutputLayers(\n",
            "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
            "      (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)\n",
            "    )\n",
            "    (mask_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (mask_head): MaskRCNNConvUpsampleHead(\n",
            "      (mask_fcn1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn2): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn3): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn4): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
            "      (deconv_relu): ReLU()\n",
            "      (predictor): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): SemSegFPNHead(\n",
            "    (p2): Sequential(\n",
            "      (0): Conv2d(\n",
            "        256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (p3): Sequential(\n",
            "      (0): Conv2d(\n",
            "        256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (1): Upsample(scale_factor=2.0, mode='bilinear')\n",
            "    )\n",
            "    (p4): Sequential(\n",
            "      (0): Conv2d(\n",
            "        256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (1): Upsample(scale_factor=2.0, mode='bilinear')\n",
            "      (2): Conv2d(\n",
            "        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (3): Upsample(scale_factor=2.0, mode='bilinear')\n",
            "    )\n",
            "    (p5): Sequential(\n",
            "      (0): Conv2d(\n",
            "        256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (1): Upsample(scale_factor=2.0, mode='bilinear')\n",
            "      (2): Conv2d(\n",
            "        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (3): Upsample(scale_factor=2.0, mode='bilinear')\n",
            "      (4): Conv2d(\n",
            "        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (5): Upsample(scale_factor=2.0, mode='bilinear')\n",
            "    )\n",
            "    (predictor): Conv2d(128, 2, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            ")\n",
            "[02/03 14:02:26 d2.data.build]: Using training sampler TrainingSampler\n",
            "[02/03 14:02:26 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[02/03 14:02:26 d2.data.common]: Serializing 50 elements to byte tensors and concatenating them all ...\n",
            "[02/03 14:02:26 d2.data.common]: Serialized dataset takes 0.19 MiB\n",
            "[02/03 14:02:26 d2.data.build]: Making batched data loader with batch_size=2\n",
            "[02/03 14:02:26 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from detectron2://ImageNetPretrained/MSRA/R-50.pkl ...\n",
            "[02/03 14:02:26 d2.checkpoint.c2_model_loading]: Renaming Caffe2 weights ......\n",
            "[02/03 14:02:26 d2.checkpoint.c2_model_loading]: Following weights matched with submodule backbone.bottom_up - Total num: 54\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:fvcore.common.checkpoint:Some model parameters or buffers are not found in the checkpoint:\n",
            "backbone.fpn_lateral2.{bias, weight}\n",
            "backbone.fpn_lateral3.{bias, weight}\n",
            "backbone.fpn_lateral4.{bias, weight}\n",
            "backbone.fpn_lateral5.{bias, weight}\n",
            "backbone.fpn_output2.{bias, weight}\n",
            "backbone.fpn_output3.{bias, weight}\n",
            "backbone.fpn_output4.{bias, weight}\n",
            "backbone.fpn_output5.{bias, weight}\n",
            "proposal_generator.rpn_head.anchor_deltas.{bias, weight}\n",
            "proposal_generator.rpn_head.conv.{bias, weight}\n",
            "proposal_generator.rpn_head.objectness_logits.{bias, weight}\n",
            "roi_heads.box_head.fc1.{bias, weight}\n",
            "roi_heads.box_head.fc2.{bias, weight}\n",
            "roi_heads.box_predictor.bbox_pred.{bias, weight}\n",
            "roi_heads.box_predictor.cls_score.{bias, weight}\n",
            "roi_heads.mask_head.deconv.{bias, weight}\n",
            "roi_heads.mask_head.mask_fcn1.{bias, weight}\n",
            "roi_heads.mask_head.mask_fcn2.{bias, weight}\n",
            "roi_heads.mask_head.mask_fcn3.{bias, weight}\n",
            "roi_heads.mask_head.mask_fcn4.{bias, weight}\n",
            "roi_heads.mask_head.predictor.{bias, weight}\n",
            "sem_seg_head.p2.0.norm.{bias, weight}\n",
            "sem_seg_head.p2.0.weight\n",
            "sem_seg_head.p3.0.norm.{bias, weight}\n",
            "sem_seg_head.p3.0.weight\n",
            "sem_seg_head.p4.0.norm.{bias, weight}\n",
            "sem_seg_head.p4.0.weight\n",
            "sem_seg_head.p4.2.norm.{bias, weight}\n",
            "sem_seg_head.p4.2.weight\n",
            "sem_seg_head.p5.0.norm.{bias, weight}\n",
            "sem_seg_head.p5.0.weight\n",
            "sem_seg_head.p5.2.norm.{bias, weight}\n",
            "sem_seg_head.p5.2.weight\n",
            "sem_seg_head.p5.4.norm.{bias, weight}\n",
            "sem_seg_head.p5.4.weight\n",
            "sem_seg_head.predictor.{bias, weight}\n",
            "WARNING:fvcore.common.checkpoint:The checkpoint state_dict contains keys that are not used by the model:\n",
            "  fc1000.{bias, weight}\n",
            "  stem.conv1.bias\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[02/03 14:02:26 d2.engine.train_loop]: Starting training from iteration 0\n",
            "[02/03 14:02:55 d2.utils.events]:  eta: 0:01:54  iter: 19  total_loss: 3.826  loss_sem_seg: 0.3491  loss_rpn_cls: 0.6887  loss_rpn_loc: 1.348  loss_cls: 0.5096  loss_box_reg: 0.03036  loss_mask: 0.688    time: 1.4322  last_time: 1.3976  data_time: 0.6117  last_data_time: 0.5133   lr: 4.7703e-05  max_mem: 5918M\n",
            "[02/03 14:03:21 d2.utils.events]:  eta: 0:01:17  iter: 39  total_loss: 3.701  loss_sem_seg: 0.4231  loss_rpn_cls: 0.6832  loss_rpn_loc: 1.22  loss_cls: 0.3396  loss_box_reg: 0.02605  loss_mask: 0.6213    time: 1.3555  last_time: 1.4998  data_time: 0.4762  last_data_time: 0.6001   lr: 9.7653e-05  max_mem: 5936M\n",
            "[02/03 14:03:50 d2.utils.events]:  eta: 0:00:51  iter: 59  total_loss: 3.575  loss_sem_seg: 0.6274  loss_rpn_cls: 0.6598  loss_rpn_loc: 1.354  loss_cls: 0.2785  loss_box_reg: 0.03731  loss_mask: 0.508    time: 1.3612  last_time: 1.3430  data_time: 0.5164  last_data_time: 0.4800   lr: 0.0001476  max_mem: 5936M\n",
            "[02/03 14:04:16 d2.utils.events]:  eta: 0:00:25  iter: 79  total_loss: 2.941  loss_sem_seg: 0.1253  loss_rpn_cls: 0.6091  loss_rpn_loc: 1.212  loss_cls: 0.3137  loss_box_reg: 0.1142  loss_mask: 0.2958    time: 1.3503  last_time: 2.0574  data_time: 0.4836  last_data_time: 0.9886   lr: 0.00019755  max_mem: 5941M\n",
            "[02/03 14:04:47 d2.utils.events]:  eta: 0:00:00  iter: 99  total_loss: 2.74  loss_sem_seg: 0.4539  loss_rpn_cls: 0.536  loss_rpn_loc: 1.248  loss_cls: 0.231  loss_box_reg: 0.1171  loss_mask: 0.1419    time: 1.3606  last_time: 1.2729  data_time: 0.5725  last_data_time: 0.4003   lr: 0.0002475  max_mem: 5941M\n",
            "[02/03 14:04:48 d2.engine.hooks]: Overall training speed: 98 iterations in 0:02:13 (1.3606 s / it)\n",
            "[02/03 14:04:48 d2.engine.hooks]: Total training time: 0:02:18 (0:00:05 on hooks)\n",
            "\n",
            "======================================================================\n",
            "‚úÖ TRAINING COMPLETE!\n",
            "======================================================================\n",
            "Model saved to: /content/output\n",
            "Check for model_final.pth\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "VERIFY"
      ],
      "metadata": {
        "id": "za6RH_PAvMDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TRAINING OUTPUTS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# List all files in output directory\n",
        "output_files = os.listdir(cfg.OUTPUT_DIR)\n",
        "print(f\"\\nFiles in {cfg.OUTPUT_DIR}:\")\n",
        "for f in sorted(output_files):\n",
        "    fpath = os.path.join(cfg.OUTPUT_DIR, f)\n",
        "    if os.path.isfile(fpath):\n",
        "        size_mb = os.path.getsize(fpath) / (1024 * 1024)\n",
        "        print(f\"  {f:<30} {size_mb:>8.2f} MB\")\n",
        "\n",
        "# Check for final model\n",
        "model_files = glob.glob(os.path.join(cfg.OUTPUT_DIR, \"model_*.pth\"))\n",
        "if model_files:\n",
        "    print(f\"\\n‚úì Found {len(model_files)} model checkpoint(s)\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  No model checkpoints found!\")\n",
        "\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Qn0Ey9avNsn",
        "outputId": "f1ef463c-058b-42e3-8ef8-cea1e0a35dfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "TRAINING OUTPUTS\n",
            "======================================================================\n",
            "\n",
            "Files in /content/output:\n",
            "  events.out.tfevents.1770126713.75f14d711bae.1168.0     0.00 MB\n",
            "  events.out.tfevents.1770127375.75f14d711bae.1168.1     0.01 MB\n",
            "  last_checkpoint                    0.00 MB\n",
            "  metrics.json                       0.00 MB\n",
            "  model_0000049.pth                347.16 MB\n",
            "  model_0000099.pth                347.16 MB\n",
            "  model_final.pth                  347.16 MB\n",
            "\n",
            "‚úì Found 3 model checkpoint(s)\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DELETE"
      ],
      "metadata": {
        "id": "Bf2oZX4zxVnV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
        "\n",
        "dataset_name = \"idd_panoptic_train\"\n",
        "\n",
        "# Remove old registration if it exists\n",
        "if dataset_name in DatasetCatalog.list():\n",
        "    DatasetCatalog.remove(dataset_name)\n",
        "    MetadataCatalog.remove(dataset_name)\n"
      ],
      "metadata": {
        "id": "wf_kZ4hSxXp_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}